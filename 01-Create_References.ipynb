{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESIP Cloud Computing Meeting Kerchunk Tutorial 2022-04-25\n",
    "__This presentation is based on work I did during the [NCAR Summer Internship in Parallel Computational Science (SIParCS) program](https://www2.cisl.ucar.edu/siparcs)__\n",
    "### Lucas Sterzinger -- Atmospheric Science PhD Candidate at UC Davis\n",
    "* [Twitter](https://twitter.com/lucassterzinger)\n",
    "* [GitHub](https://github.com/lsterzinger)\n",
    "* [Website](https://lucassterzinger.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Motivation:\n",
    "* NetCDF is not cloud optimized\n",
    "* Other formats, like Zarr, aim to make accessing and reading data from the cloud fast and painless\n",
    "* However, most geoscience datasets available in the cloud are still in their native NetCDF/HDF5, so a different access method is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do I mean when I say \"Cloud Optimized\"?\n",
    "![Move to cloud diagram](images/cloud-move.png)\n",
    "\n",
    "In traditional scientific workflows, data is archived in a repository and downloaded to a separate computer for analysis (left). However, datasets are becoming much too large to fit on personal computers, and transferring full datasets from an archive to a seperate machine can use lots of bandwidth.\n",
    "\n",
    "In a cloud environment, the data can live in object storage (e.g. AWS S3), and analysis can be done in an adjacent compute instances, allowing for low-latency and high-bandwith access to the dataset.\n",
    "\n",
    "## Why NetCDF doesn't work well in this workflow\n",
    "\n",
    "NetCDF is probably the most common binary data format for atmospheric/earth sciences, and has a lot of official and community support. However, the NetCDF format/API requires either a) many small reads to access the metadata for a single file or b) use a serverside utility like THREDDS/OPeNDAP to extract metadata.\n",
    "\n",
    "![NetCDF File Object](images/single_file_object.png)\n",
    "\n",
    "## The Zarr Solution\n",
    "The [Zarr data format](https://zarr.readthedocs.io/en/stable/) alleviates this problem by storing the metadata and chunks in seperate files that can be accessed as-needed and in parallel. Having consolidated metadata means that all the information about the dataset can be loaded and interpreted in a single read of a small plaintext file. With this metadata in-hand, a program can request exactly which chunks of data are needed for a given operation.\n",
    "\n",
    "![Zarr](images/zarr.png)\n",
    "\n",
    "## _However_\n",
    "While Zarr proves to be very good for this cloud-centric workflow, most cloud-available data is currently only available in NetCDF/HDF5/GRIB2 format. While it would be _wonderful_ if all this data converted to Zarr overnight, it would be great if in the meantime there was a way to use some of the Zarr spec, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducting `kerchunk`\n",
    "[Github page](https://github.com/intake/kerchunk)\n",
    "\n",
    "`kerchunk` works by doing all the heavy lifting of extracting the metadata, generating byte-ranges for each variable chunk, and creating a Zarr-spec metadata file. This file is plaintext and can opened and analyzed with xarray very quickly. When a user requests a certain chunk of data, the NetCDF4 API is bypassed entirely and the Zarr API is used to extract the specified byte-range.\n",
    "\n",
    "![reference-maker vs zarr](images/referencemaker_v_zarr.png)\n",
    "\n",
    "## How much of a difference does this make, really?\n",
    "Testing this method on workflow processing of 24 hours of 5-minute GOES-16 data and accessing via native NetCDF, Zarr, and NetCDF + ReferenceMaker:\n",
    "\n",
    "![workflow results](images/workflow_results.png)\n",
    "\n",
    "*Notebooks used to benchmark these times are available here: https://github.com/lsterzinger/cloud-optimized-satellite-data-tests*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import `kerchunk` and make sure it's at the latest version (`0.0.6` at the time of writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerchunk\n",
    "kerchunk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If Kerchunk is not at the latest version, update with pip/conda: and **restart the kernel**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade kerchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import fsspec\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fsspec` -- What is it?\n",
    "* Provides unified interface to different filesystem types\n",
    "* Local, cloud, http, dropbox, Google Drive, etc\n",
    "    * All accessible with the same API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsspec.registry import known_implementations\n",
    "known_implementations.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open a new filesystem, of type `s3` (Amazon Web Services storage)\n",
    "This tells `fsspec` what type of storage system to use (AWS S3) and any authentication options (this is a public dataset, so use anonymous mode `anon=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `fs.glob()` to generate a list of files in a certain directory. Goes data is stored in `s3://noaa-goes16/<product>/<year>/<day_of_year>/<hour>/<datafile>.nc` format.\n",
    "\n",
    "This `glob()` returns all files in the 210th day of 2020 (July 28th, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = fs.glob(\"s3://noaa-goes16/ABI-L2-SSTF/2020/210/*/*.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepend `s3://` to the URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = ['s3://' + f for f in flist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of creating a single reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = flist[0]\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(u, mode=\"rb\", anon=True) as infile:\n",
    "    reference = SingleHdf5ToZarr(infile, u, inline_threshold=100).translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create references for all 24 files in `flist`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## With a local Dask cluster\n",
    "[Dask](https://dask.org/) is a python package that allows for easily parallelizing python code. This section starts a local client (using whatever processors are available on the current machine). This can also be done just as easily using [Dask clusters](https://docs.dask.org/en/stable/deploying.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ESIP Jupyterhub\n",
    "\n",
    "This should be used instead of the above local Dask cluster if running on ESIP's JupyterHub at https://jupyter.qhub.esipfed.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "# import ebdpy as ebd\n",
    "\n",
    "# ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "# profile = 'esip-qhub'\n",
    "# region = 'us-west-2'\n",
    "# endpoint = f's3.{region}.amazonaws.com'\n",
    "# ebd.set_credentials(profile=profile, region=region, endpoint=endpoint)\n",
    "# worker_max = 10\n",
    "# client,cluster = ebd.start_dask_cluster(profile=profile,worker_max=worker_max, \n",
    "#                                       region=region, use_existing_cluster=True,\n",
    "#                                       adaptive_scaling=False, wait_for_cluster=False, \n",
    "#                                       environment='pangeo', worker_profile='Pangeo Worker', \n",
    "#                                       propagate_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definte function to return a reference dictionary for a given S3 file URL\n",
    "\n",
    "This function does the following:\n",
    "1. `so` is a dictionary of options for `fsspec.open()`\n",
    "2. Use `fsspec.open()` to open the file given by URL `f`\n",
    "3. Using `kerchunk.SingleHdf5ToZarr()` and supplying the file object `infile` and URL `f`, generate reference with `.translate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ref(f):\n",
    "    with fsspec.open(f, mode=\"rb\", anon=True) as infile:\n",
    "        return SingleHdf5ToZarr(infile, f, inline_threshold=300).translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Map `gen_ref` to each member of `flist_bag` and compute\n",
    "Dask bag is a way to map a function to a set of inputs. This next couple blocks of code tell Dask to take all the files in `flist`, break them up into the same amount of partitions and map each partition to the `gen_ref()` function -- essentially mapping each file path to `gen_ref()`. Calling `bag.compute()` on this runs `gen_ref()` in parallel with as many workers as are available in Dask client.\n",
    "\n",
    "_Note: if running interactively on Binder, this will take a while since only one worker is available and the references will have to be generated in serial. See option for loading from jsons below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "bag = db.from_sequence(flist, npartitions=len(flist)).map(gen_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dicts = bag.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each url in `flist` has been used to generate a dictionary of reference data in `dicts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _Save/load references to/from JSON files (optional)_\n",
    "The individual dictionaries can be saved as JSON files if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ujson\n",
    "# for d in dicts:\n",
    "#     # Generate name from corresponding URL:\n",
    "#     # Grab URL, strip everything but the filename, \n",
    "#     # and replace .nc with .json\n",
    "#     name = './example_jsons/individual/'+ d['templates']['u'].split('/')[-1].replace('.nc', '.json')\n",
    "\n",
    "#     with open(name, 'w') as outf:\n",
    "#         outf.write(ujson.dumps(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These generated jsons can then be loaded back in as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ujson\n",
    "# dicts = []\n",
    "\n",
    "# for f in sorted(glob('./example_jsons/individual/*.json')):\n",
    "#     with open(f,'r') as fin:\n",
    "#         dicts.append(ujson.load(fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Use `MultiZarrToZarr` to combine the 24 individual references into a single reference\n",
    "In this example we passed a list of reference dictionaries, but you can also give it a list of `.json` filepaths (commented out)\n",
    "\n",
    "_Note: the Kerchunk `MultiZarrToZarr` API changed between versions 0.0.5 and 0.0.6. This part assumes the latest version at this time (0.0.6). Please see https://fsspec.github.io/kerchunk/reference.html#kerchunk.combine.MultiZarrToZarr for more details_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz = MultiZarrToZarr(\n",
    "    dicts,\n",
    "    # sorted((glob('./example_jsons/individual/*.json'))),\n",
    "    remote_protocol='s3',\n",
    "    remote_options={'anon':True},\n",
    "    concat_dims='t',\n",
    "    inline_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References can be saved to a file (`combined.json`) or passed back as a dictionary (`mzz_dict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mzz.translate('./combined.json')\n",
    "# mzz_dict = mzz.translate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d853cbf2f35f45a59f79ca5e397d8dd1594080251b0b51418fe33f5fb0138a7a"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
